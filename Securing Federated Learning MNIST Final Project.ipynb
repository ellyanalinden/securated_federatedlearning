{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0802 14:42:09.632205  1724 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was 'C:\\Users\\Vilas_2\\Anaconda3\\envs\\pysyft\\lib\\site-packages\\tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
      "W0802 14:42:09.727210  1724 deprecation_wrapper.py:119] From C:\\Users\\Vilas_2\\Anaconda3\\envs\\pysyft\\lib\\site-packages\\tf_encrypted\\session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import syft as sy\n",
    "hook = sy.TorchHook(torch)\n",
    "client = sy.VirtualWorker(hook, id='client')\n",
    "bob = sy.VirtualWorker(hook, id='bob')\n",
    "alice = sy.VirtualWorker(hook, id='alice')\n",
    "crypto_provider = sy.VirtualWorker(hook, id='crypto_provider')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the learning task\n",
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 50\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.001\n",
    "        self.log_interval = 100\n",
    "        \n",
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loading and sending to workers\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                datasets.MNIST('~/.pytorch/MNIST_data/', train=True, download=True,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,))\n",
    "                              ])),\n",
    "                batch_size=args.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, client has some data and would like to have predictions on it using the server's model. This client encrypts its data by sharing it additively across two workers alice and bob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMPC uses crypto protocols which require to work on integers. We leverage here the pysyft tensor abstraction to convert PyTorch Float tensors into Fixed Precission Tensors using .fix_precision(). \n",
    "For example 0.123 with precission 2 does a rounding at the 2nd decimal digit so the number stored is the integer 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('~/.pytorch/MNIST_data/', train=False,\n",
    "                  transform=transforms.Compose([\n",
    "                      transforms.ToTensor(),\n",
    "                      transforms.Normalize((0.1307,), (0.3081,))\n",
    "                  ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True)\n",
    "\n",
    "private_test_loader = []\n",
    "for data, target in test_loader:\n",
    "    private_test_loader.append((\n",
    "    data.fix_precision().share(alice, bob, crypto_provider=crypto_provider),\n",
    "    target.fix_precision().share(alice, bob, crypto_provider = crypto_provider)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feed forward NN\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Launch the training\n",
    "def train(args, model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data,target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval==0:\n",
    "            print('Train Epoch:{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * args.batch_size, len(train_loader) * args.batch_size,\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:1 [0/60032 (0%)]\tLoss: 2.337576\n",
      "Train Epoch:1 [6400/60032 (11%)]\tLoss: 0.140855\n",
      "Train Epoch:1 [12800/60032 (21%)]\tLoss: 0.210901\n",
      "Train Epoch:1 [19200/60032 (32%)]\tLoss: 0.128651\n",
      "Train Epoch:1 [25600/60032 (43%)]\tLoss: 0.337340\n",
      "Train Epoch:1 [32000/60032 (53%)]\tLoss: 0.130668\n",
      "Train Epoch:1 [38400/60032 (64%)]\tLoss: 0.189305\n",
      "Train Epoch:1 [44800/60032 (75%)]\tLoss: 0.138222\n",
      "Train Epoch:1 [51200/60032 (85%)]\tLoss: 0.170740\n",
      "Train Epoch:1 [57600/60032 (96%)]\tLoss: 0.018991\n",
      "Train Epoch:2 [0/60032 (0%)]\tLoss: 0.051261\n",
      "Train Epoch:2 [6400/60032 (11%)]\tLoss: 0.041990\n",
      "Train Epoch:2 [12800/60032 (21%)]\tLoss: 0.065655\n",
      "Train Epoch:2 [19200/60032 (32%)]\tLoss: 0.189060\n",
      "Train Epoch:2 [25600/60032 (43%)]\tLoss: 0.168477\n",
      "Train Epoch:2 [32000/60032 (53%)]\tLoss: 0.150479\n",
      "Train Epoch:2 [38400/60032 (64%)]\tLoss: 0.062504\n",
      "Train Epoch:2 [44800/60032 (75%)]\tLoss: 0.060785\n",
      "Train Epoch:2 [51200/60032 (85%)]\tLoss: 0.012393\n",
      "Train Epoch:2 [57600/60032 (96%)]\tLoss: 0.111114\n",
      "Train Epoch:3 [0/60032 (0%)]\tLoss: 0.025001\n",
      "Train Epoch:3 [6400/60032 (11%)]\tLoss: 0.013272\n",
      "Train Epoch:3 [12800/60032 (21%)]\tLoss: 0.209745\n",
      "Train Epoch:3 [19200/60032 (32%)]\tLoss: 0.161365\n",
      "Train Epoch:3 [25600/60032 (43%)]\tLoss: 0.055097\n",
      "Train Epoch:3 [32000/60032 (53%)]\tLoss: 0.103473\n",
      "Train Epoch:3 [38400/60032 (64%)]\tLoss: 0.198210\n",
      "Train Epoch:3 [44800/60032 (75%)]\tLoss: 0.007381\n",
      "Train Epoch:3 [51200/60032 (85%)]\tLoss: 0.148932\n",
      "Train Epoch:3 [57600/60032 (96%)]\tLoss: 0.004873\n",
      "Train Epoch:4 [0/60032 (0%)]\tLoss: 0.018236\n",
      "Train Epoch:4 [6400/60032 (11%)]\tLoss: 0.045994\n",
      "Train Epoch:4 [12800/60032 (21%)]\tLoss: 0.026580\n",
      "Train Epoch:4 [19200/60032 (32%)]\tLoss: 0.075459\n",
      "Train Epoch:4 [25600/60032 (43%)]\tLoss: 0.074109\n",
      "Train Epoch:4 [32000/60032 (53%)]\tLoss: 0.017418\n",
      "Train Epoch:4 [38400/60032 (64%)]\tLoss: 0.054258\n",
      "Train Epoch:4 [44800/60032 (75%)]\tLoss: 0.077719\n",
      "Train Epoch:4 [51200/60032 (85%)]\tLoss: 0.043709\n",
      "Train Epoch:4 [57600/60032 (96%)]\tLoss: 0.005965\n",
      "Train Epoch:5 [0/60032 (0%)]\tLoss: 0.038969\n",
      "Train Epoch:5 [6400/60032 (11%)]\tLoss: 0.003427\n",
      "Train Epoch:5 [12800/60032 (21%)]\tLoss: 0.028092\n",
      "Train Epoch:5 [19200/60032 (32%)]\tLoss: 0.047639\n",
      "Train Epoch:5 [25600/60032 (43%)]\tLoss: 0.004452\n",
      "Train Epoch:5 [32000/60032 (53%)]\tLoss: 0.022685\n",
      "Train Epoch:5 [38400/60032 (64%)]\tLoss: 0.088307\n",
      "Train Epoch:5 [44800/60032 (75%)]\tLoss: 0.001231\n",
      "Train Epoch:5 [51200/60032 (85%)]\tLoss: 0.020995\n",
      "Train Epoch:5 [57600/60032 (96%)]\tLoss: 0.053361\n",
      "Train Epoch:6 [0/60032 (0%)]\tLoss: 0.003900\n",
      "Train Epoch:6 [6400/60032 (11%)]\tLoss: 0.005545\n",
      "Train Epoch:6 [12800/60032 (21%)]\tLoss: 0.006748\n",
      "Train Epoch:6 [19200/60032 (32%)]\tLoss: 0.008140\n",
      "Train Epoch:6 [25600/60032 (43%)]\tLoss: 0.009454\n",
      "Train Epoch:6 [32000/60032 (53%)]\tLoss: 0.030996\n",
      "Train Epoch:6 [38400/60032 (64%)]\tLoss: 0.065379\n",
      "Train Epoch:6 [44800/60032 (75%)]\tLoss: 0.036909\n",
      "Train Epoch:6 [51200/60032 (85%)]\tLoss: 0.009597\n",
      "Train Epoch:6 [57600/60032 (96%)]\tLoss: 0.012007\n",
      "Train Epoch:7 [0/60032 (0%)]\tLoss: 0.009232\n",
      "Train Epoch:7 [6400/60032 (11%)]\tLoss: 0.000829\n",
      "Train Epoch:7 [12800/60032 (21%)]\tLoss: 0.003934\n",
      "Train Epoch:7 [19200/60032 (32%)]\tLoss: 0.000550\n",
      "Train Epoch:7 [25600/60032 (43%)]\tLoss: 0.003799\n",
      "Train Epoch:7 [32000/60032 (53%)]\tLoss: 0.031615\n",
      "Train Epoch:7 [38400/60032 (64%)]\tLoss: 0.000823\n",
      "Train Epoch:7 [44800/60032 (75%)]\tLoss: 0.007179\n",
      "Train Epoch:7 [51200/60032 (85%)]\tLoss: 0.010414\n",
      "Train Epoch:7 [57600/60032 (96%)]\tLoss: 0.018517\n",
      "Train Epoch:8 [0/60032 (0%)]\tLoss: 0.012651\n",
      "Train Epoch:8 [6400/60032 (11%)]\tLoss: 0.037193\n",
      "Train Epoch:8 [12800/60032 (21%)]\tLoss: 0.001011\n",
      "Train Epoch:8 [19200/60032 (32%)]\tLoss: 0.001788\n",
      "Train Epoch:8 [25600/60032 (43%)]\tLoss: 0.092125\n",
      "Train Epoch:8 [32000/60032 (53%)]\tLoss: 0.014442\n",
      "Train Epoch:8 [38400/60032 (64%)]\tLoss: 0.021419\n",
      "Train Epoch:8 [44800/60032 (75%)]\tLoss: 0.004025\n",
      "Train Epoch:8 [51200/60032 (85%)]\tLoss: 0.040381\n",
      "Train Epoch:8 [57600/60032 (96%)]\tLoss: 0.010321\n",
      "Train Epoch:9 [0/60032 (0%)]\tLoss: 0.021235\n",
      "Train Epoch:9 [6400/60032 (11%)]\tLoss: 0.030918\n",
      "Train Epoch:9 [12800/60032 (21%)]\tLoss: 0.000076\n",
      "Train Epoch:9 [19200/60032 (32%)]\tLoss: 0.002396\n",
      "Train Epoch:9 [25600/60032 (43%)]\tLoss: 0.019809\n",
      "Train Epoch:9 [32000/60032 (53%)]\tLoss: 0.014843\n",
      "Train Epoch:9 [38400/60032 (64%)]\tLoss: 0.049883\n",
      "Train Epoch:9 [44800/60032 (75%)]\tLoss: 0.002837\n",
      "Train Epoch:9 [51200/60032 (85%)]\tLoss: 0.012611\n",
      "Train Epoch:9 [57600/60032 (96%)]\tLoss: 0.000695\n",
      "Train Epoch:10 [0/60032 (0%)]\tLoss: 0.007866\n",
      "Train Epoch:10 [6400/60032 (11%)]\tLoss: 0.003772\n",
      "Train Epoch:10 [12800/60032 (21%)]\tLoss: 0.008707\n",
      "Train Epoch:10 [19200/60032 (32%)]\tLoss: 0.003201\n",
      "Train Epoch:10 [25600/60032 (43%)]\tLoss: 0.011742\n",
      "Train Epoch:10 [32000/60032 (53%)]\tLoss: 0.000905\n",
      "Train Epoch:10 [38400/60032 (64%)]\tLoss: 0.000687\n",
      "Train Epoch:10 [44800/60032 (75%)]\tLoss: 0.001199\n",
      "Train Epoch:10 [51200/60032 (85%)]\tLoss: 0.024377\n",
      "Train Epoch:10 [57600/60032 (96%)]\tLoss: 0.010640\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, train_loader, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            output = F.log_softmax(output, dim=1)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() #sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) #get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy:{}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0755, Accuracy:9820/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(args, model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is now trained and ready to be provided as service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secure Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=784, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fix_precision().share(alice, bob, crypto_provider=crypto_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, test_loader):\n",
    "    model.eval()\n",
    "    n_correct_priv = 0\n",
    "    n_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            n_correct_priv += pred.eq(target.view_as(pred)).sum()\n",
    "            n_total += args.test_batch_size\n",
    "            \n",
    "#This 'test' fc performs the encrypted evaluation. The model weights, the data inputs, the prediction \n",
    "#and the target used for scoring are all encrypted\n",
    "\n",
    "#However as you can observe, the syntax is very similar to normal Pytorch testing\n",
    "\n",
    "#The only thing we decrypt from the server side is the final score at the end of our 200 items batches\n",
    "#to verify predictions were on average good\n",
    "\n",
    "            n_correct = n_correct_priv.copy().get().float_precision().long().item()\n",
    "            \n",
    "            print('Test set: Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "                 n_correct, n_total,\n",
    "                 100. * n_correct/ n_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Accuracy: 50/50 (100%)\n",
      "Test set: Accuracy: 96/100 (96%)\n",
      "Test set: Accuracy: 146/150 (97%)\n",
      "Test set: Accuracy: 195/200 (98%)\n",
      "Test set: Accuracy: 245/250 (98%)\n",
      "Test set: Accuracy: 294/300 (98%)\n",
      "Test set: Accuracy: 343/350 (98%)\n",
      "Test set: Accuracy: 391/400 (98%)\n",
      "Test set: Accuracy: 441/450 (98%)\n",
      "Test set: Accuracy: 491/500 (98%)\n",
      "Test set: Accuracy: 540/550 (98%)\n",
      "Test set: Accuracy: 589/600 (98%)\n",
      "Test set: Accuracy: 639/650 (98%)\n",
      "Test set: Accuracy: 687/700 (98%)\n",
      "Test set: Accuracy: 736/750 (98%)\n",
      "Test set: Accuracy: 786/800 (98%)\n",
      "Test set: Accuracy: 836/850 (98%)\n",
      "Test set: Accuracy: 886/900 (98%)\n",
      "Test set: Accuracy: 936/950 (99%)\n",
      "Test set: Accuracy: 983/1000 (98%)\n",
      "Test set: Accuracy: 1033/1050 (98%)\n",
      "Test set: Accuracy: 1082/1100 (98%)\n",
      "Test set: Accuracy: 1132/1150 (98%)\n",
      "Test set: Accuracy: 1181/1200 (98%)\n",
      "Test set: Accuracy: 1230/1250 (98%)\n",
      "Test set: Accuracy: 1279/1300 (98%)\n",
      "Test set: Accuracy: 1328/1350 (98%)\n",
      "Test set: Accuracy: 1375/1400 (98%)\n",
      "Test set: Accuracy: 1424/1450 (98%)\n",
      "Test set: Accuracy: 1473/1500 (98%)\n",
      "Test set: Accuracy: 1520/1550 (98%)\n",
      "Test set: Accuracy: 1569/1600 (98%)\n",
      "Test set: Accuracy: 1617/1650 (98%)\n",
      "Test set: Accuracy: 1666/1700 (98%)\n",
      "Test set: Accuracy: 1715/1750 (98%)\n",
      "Test set: Accuracy: 1765/1800 (98%)\n",
      "Test set: Accuracy: 1814/1850 (98%)\n",
      "Test set: Accuracy: 1863/1900 (98%)\n",
      "Test set: Accuracy: 1912/1950 (98%)\n",
      "Test set: Accuracy: 1961/2000 (98%)\n",
      "Test set: Accuracy: 2010/2050 (98%)\n",
      "Test set: Accuracy: 2060/2100 (98%)\n",
      "Test set: Accuracy: 2108/2150 (98%)\n",
      "Test set: Accuracy: 2158/2200 (98%)\n",
      "Test set: Accuracy: 2205/2250 (98%)\n",
      "Test set: Accuracy: 2254/2300 (98%)\n",
      "Test set: Accuracy: 2303/2350 (98%)\n",
      "Test set: Accuracy: 2352/2400 (98%)\n",
      "Test set: Accuracy: 2402/2450 (98%)\n",
      "Test set: Accuracy: 2452/2500 (98%)\n",
      "Test set: Accuracy: 2501/2550 (98%)\n",
      "Test set: Accuracy: 2551/2600 (98%)\n",
      "Test set: Accuracy: 2600/2650 (98%)\n",
      "Test set: Accuracy: 2650/2700 (98%)\n",
      "Test set: Accuracy: 2699/2750 (98%)\n",
      "Test set: Accuracy: 2747/2800 (98%)\n",
      "Test set: Accuracy: 2796/2850 (98%)\n",
      "Test set: Accuracy: 2846/2900 (98%)\n",
      "Test set: Accuracy: 2896/2950 (98%)\n",
      "Test set: Accuracy: 2946/3000 (98%)\n",
      "Test set: Accuracy: 2996/3050 (98%)\n",
      "Test set: Accuracy: 3046/3100 (98%)\n",
      "Test set: Accuracy: 3096/3150 (98%)\n",
      "Test set: Accuracy: 3143/3200 (98%)\n",
      "Test set: Accuracy: 3192/3250 (98%)\n",
      "Test set: Accuracy: 3242/3300 (98%)\n",
      "Test set: Accuracy: 3290/3350 (98%)\n",
      "Test set: Accuracy: 3339/3400 (98%)\n",
      "Test set: Accuracy: 3389/3450 (98%)\n",
      "Test set: Accuracy: 3439/3500 (98%)\n",
      "Test set: Accuracy: 3489/3550 (98%)\n",
      "Test set: Accuracy: 3538/3600 (98%)\n",
      "Test set: Accuracy: 3588/3650 (98%)\n",
      "Test set: Accuracy: 3638/3700 (98%)\n",
      "Test set: Accuracy: 3687/3750 (98%)\n",
      "Test set: Accuracy: 3735/3800 (98%)\n",
      "Test set: Accuracy: 3782/3850 (98%)\n",
      "Test set: Accuracy: 3828/3900 (98%)\n",
      "Test set: Accuracy: 3878/3950 (98%)\n",
      "Test set: Accuracy: 3926/4000 (98%)\n",
      "Test set: Accuracy: 3973/4050 (98%)\n",
      "Test set: Accuracy: 4022/4100 (98%)\n",
      "Test set: Accuracy: 4072/4150 (98%)\n",
      "Test set: Accuracy: 4122/4200 (98%)\n",
      "Test set: Accuracy: 4171/4250 (98%)\n",
      "Test set: Accuracy: 4220/4300 (98%)\n",
      "Test set: Accuracy: 4269/4350 (98%)\n",
      "Test set: Accuracy: 4319/4400 (98%)\n",
      "Test set: Accuracy: 4366/4450 (98%)\n",
      "Test set: Accuracy: 4413/4500 (98%)\n",
      "Test set: Accuracy: 4463/4550 (98%)\n",
      "Test set: Accuracy: 4511/4600 (98%)\n",
      "Test set: Accuracy: 4560/4650 (98%)\n",
      "Test set: Accuracy: 4610/4700 (98%)\n",
      "Test set: Accuracy: 4660/4750 (98%)\n",
      "Test set: Accuracy: 4709/4800 (98%)\n",
      "Test set: Accuracy: 4758/4850 (98%)\n",
      "Test set: Accuracy: 4808/4900 (98%)\n",
      "Test set: Accuracy: 4857/4950 (98%)\n",
      "Test set: Accuracy: 4907/5000 (98%)\n",
      "Test set: Accuracy: 4955/5050 (98%)\n",
      "Test set: Accuracy: 5005/5100 (98%)\n",
      "Test set: Accuracy: 5055/5150 (98%)\n",
      "Test set: Accuracy: 5103/5200 (98%)\n",
      "Test set: Accuracy: 5153/5250 (98%)\n",
      "Test set: Accuracy: 5199/5300 (98%)\n",
      "Test set: Accuracy: 5247/5350 (98%)\n",
      "Test set: Accuracy: 5297/5400 (98%)\n",
      "Test set: Accuracy: 5345/5450 (98%)\n",
      "Test set: Accuracy: 5395/5500 (98%)\n",
      "Test set: Accuracy: 5444/5550 (98%)\n",
      "Test set: Accuracy: 5494/5600 (98%)\n",
      "Test set: Accuracy: 5544/5650 (98%)\n",
      "Test set: Accuracy: 5594/5700 (98%)\n",
      "Test set: Accuracy: 5644/5750 (98%)\n",
      "Test set: Accuracy: 5692/5800 (98%)\n",
      "Test set: Accuracy: 5742/5850 (98%)\n",
      "Test set: Accuracy: 5792/5900 (98%)\n",
      "Test set: Accuracy: 5840/5950 (98%)\n",
      "Test set: Accuracy: 5890/6000 (98%)\n",
      "Test set: Accuracy: 5937/6050 (98%)\n",
      "Test set: Accuracy: 5987/6100 (98%)\n",
      "Test set: Accuracy: 6036/6150 (98%)\n",
      "Test set: Accuracy: 6084/6200 (98%)\n",
      "Test set: Accuracy: 6132/6250 (98%)\n",
      "Test set: Accuracy: 6182/6300 (98%)\n",
      "Test set: Accuracy: 6231/6350 (98%)\n",
      "Test set: Accuracy: 6280/6400 (98%)\n",
      "Test set: Accuracy: 6329/6450 (98%)\n",
      "Test set: Accuracy: 6377/6500 (98%)\n",
      "Test set: Accuracy: 6427/6550 (98%)\n",
      "Test set: Accuracy: 6477/6600 (98%)\n",
      "Test set: Accuracy: 6526/6650 (98%)\n",
      "Test set: Accuracy: 6575/6700 (98%)\n",
      "Test set: Accuracy: 6624/6750 (98%)\n",
      "Test set: Accuracy: 6674/6800 (98%)\n",
      "Test set: Accuracy: 6722/6850 (98%)\n",
      "Test set: Accuracy: 6772/6900 (98%)\n",
      "Test set: Accuracy: 6821/6950 (98%)\n",
      "Test set: Accuracy: 6868/7000 (98%)\n",
      "Test set: Accuracy: 6917/7050 (98%)\n",
      "Test set: Accuracy: 6966/7100 (98%)\n",
      "Test set: Accuracy: 7015/7150 (98%)\n",
      "Test set: Accuracy: 7064/7200 (98%)\n",
      "Test set: Accuracy: 7114/7250 (98%)\n",
      "Test set: Accuracy: 7164/7300 (98%)\n",
      "Test set: Accuracy: 7214/7350 (98%)\n",
      "Test set: Accuracy: 7263/7400 (98%)\n",
      "Test set: Accuracy: 7312/7450 (98%)\n",
      "Test set: Accuracy: 7361/7500 (98%)\n",
      "Test set: Accuracy: 7410/7550 (98%)\n",
      "Test set: Accuracy: 7460/7600 (98%)\n",
      "Test set: Accuracy: 7510/7650 (98%)\n",
      "Test set: Accuracy: 7557/7700 (98%)\n",
      "Test set: Accuracy: 7606/7750 (98%)\n",
      "Test set: Accuracy: 7654/7800 (98%)\n",
      "Test set: Accuracy: 7703/7850 (98%)\n",
      "Test set: Accuracy: 7753/7900 (98%)\n",
      "Test set: Accuracy: 7802/7950 (98%)\n",
      "Test set: Accuracy: 7852/8000 (98%)\n",
      "Test set: Accuracy: 7901/8050 (98%)\n",
      "Test set: Accuracy: 7950/8100 (98%)\n",
      "Test set: Accuracy: 7999/8150 (98%)\n",
      "Test set: Accuracy: 8048/8200 (98%)\n",
      "Test set: Accuracy: 8097/8250 (98%)\n",
      "Test set: Accuracy: 8146/8300 (98%)\n",
      "Test set: Accuracy: 8195/8350 (98%)\n",
      "Test set: Accuracy: 8244/8400 (98%)\n",
      "Test set: Accuracy: 8290/8450 (98%)\n",
      "Test set: Accuracy: 8340/8500 (98%)\n",
      "Test set: Accuracy: 8386/8550 (98%)\n",
      "Test set: Accuracy: 8435/8600 (98%)\n",
      "Test set: Accuracy: 8484/8650 (98%)\n",
      "Test set: Accuracy: 8534/8700 (98%)\n",
      "Test set: Accuracy: 8584/8750 (98%)\n",
      "Test set: Accuracy: 8633/8800 (98%)\n",
      "Test set: Accuracy: 8682/8850 (98%)\n",
      "Test set: Accuracy: 8732/8900 (98%)\n",
      "Test set: Accuracy: 8781/8950 (98%)\n",
      "Test set: Accuracy: 8831/9000 (98%)\n",
      "Test set: Accuracy: 8880/9050 (98%)\n",
      "Test set: Accuracy: 8929/9100 (98%)\n",
      "Test set: Accuracy: 8979/9150 (98%)\n",
      "Test set: Accuracy: 9028/9200 (98%)\n",
      "Test set: Accuracy: 9078/9250 (98%)\n",
      "Test set: Accuracy: 9128/9300 (98%)\n",
      "Test set: Accuracy: 9177/9350 (98%)\n",
      "Test set: Accuracy: 9227/9400 (98%)\n",
      "Test set: Accuracy: 9276/9450 (98%)\n",
      "Test set: Accuracy: 9325/9500 (98%)\n",
      "Test set: Accuracy: 9375/9550 (98%)\n",
      "Test set: Accuracy: 9424/9600 (98%)\n",
      "Test set: Accuracy: 9474/9650 (98%)\n",
      "Test set: Accuracy: 9523/9700 (98%)\n",
      "Test set: Accuracy: 9572/9750 (98%)\n",
      "Test set: Accuracy: 9622/9800 (98%)\n",
      "Test set: Accuracy: 9672/9850 (98%)\n",
      "Test set: Accuracy: 9721/9900 (98%)\n",
      "Test set: Accuracy: 9770/9950 (98%)\n",
      "Test set: Accuracy: 9820/10000 (98%)\n"
     ]
    }
   ],
   "source": [
    "test(args, model, private_test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
